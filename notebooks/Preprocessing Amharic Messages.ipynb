{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(r\"C:\\Users\\HP\\week 5\\Amharic-Named-Entity-Recognition-\\data\\scraped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and tokenize Amharic text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Ensure that text is a string\n",
    "        # Remove non-Amharic characters and emojis\n",
    "        cleaned_text = re.sub(r'[^\\u1200-\\u137F ]+', '', text)  # Amharic Unicode range\n",
    "        # Tokenize the text by splitting based on spaces\n",
    "        tokens = cleaned_text.split()\n",
    "        return tokens\n",
    "    else:\n",
    "        return []  # Return an empty list if the message is not a string\n",
    "\n",
    "# Apply the preprocessing function\n",
    "df['tokens'] = df['message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled data saved successfully.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ“£ ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” \\n\\nğŸ“ á‹­áˆ„áŠ•áŠ•ğŸ‘‰ t.me/MerttEka ğŸ‘ˆ á‰°áŒ­áŠá‹Â  ...</td>\n",
       "      <td>[á‹­áˆ„áŠ•áŠ•, á‰°áŒ­áŠá‹, á‹«á‹µáˆ­áŒ‰á£, á‰¤á‰°áˆ°á‰¥]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ğŸ“£ MiNI HUMIDIFIER\\n\\nâœ”ï¸ á‹¨á‰¤á‰µ á‹áˆµáŒ¥ áˆ˜á‹“á‹› áˆ›áŒ¨áˆ»\\nâœ”ï¸ á‰¤á‰µ...</td>\n",
       "      <td>[á‹¨á‰¤á‰µ, á‹áˆµáŒ¥, áˆ˜á‹“á‹›, áˆ›áŒ¨áˆ», á‰¤á‰µá‹áŠ•á‰¢áˆ®á‹áŠ•, á‰ áˆ˜áˆáŠ«áˆ, á‹˜áˆ¨áŠ•, á‹«á‹á‹±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ğŸ“£  ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ”   Silicone Baking Mould\\n\\nâœ”ï¸ 12 á‹¨áŠ©áŠª...</td>\n",
       "      <td>[á‹¨áŠ©áŠªáˆµ, áˆ˜áˆµáˆªá‹«, áˆáˆá‹¶á‰½, áŠ áˆ«á‰µ, áŠ á‹­áŠá‰µ, áˆ†áŠ–, áŠ¥á‹«áŠ•á‹³áŠ•á‹±, á‰…áˆ­á…,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  ğŸ“£ ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” \\n\\nğŸ“ á‹­áˆ„áŠ•áŠ•ğŸ‘‰ t.me/MerttEka ğŸ‘ˆ á‰°áŒ­áŠá‹Â  ...   \n",
       "1  ğŸ“£ MiNI HUMIDIFIER\\n\\nâœ”ï¸ á‹¨á‰¤á‰µ á‹áˆµáŒ¥ áˆ˜á‹“á‹› áˆ›áŒ¨áˆ»\\nâœ”ï¸ á‰¤á‰µ...   \n",
       "2  ğŸ“£  ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ” ğŸ”   Silicone Baking Mould\\n\\nâœ”ï¸ 12 á‹¨áŠ©áŠª...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              tokens  \n",
       "0                          [á‹­áˆ„áŠ•áŠ•, á‰°áŒ­áŠá‹, á‹«á‹µáˆ­áŒ‰á£, á‰¤á‰°áˆ°á‰¥]  \n",
       "1  [á‹¨á‰¤á‰µ, á‹áˆµáŒ¥, áˆ˜á‹“á‹›, áˆ›áŒ¨áˆ», á‰¤á‰µá‹áŠ•á‰¢áˆ®á‹áŠ•, á‰ áˆ˜áˆáŠ«áˆ, á‹˜áˆ¨áŠ•, á‹«á‹á‹±...  \n",
       "2  [á‹¨áŠ©áŠªáˆµ, áˆ˜áˆµáˆªá‹«, áˆáˆá‹¶á‰½, áŠ áˆ«á‰µ, áŠ á‹­áŠá‰µ, áˆ†áŠ–, áŠ¥á‹«áŠ•á‹³áŠ•á‹±, á‰…áˆ­á…,...  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\HP\\week 5\\Amharic-Named-Entity-Recognition-\\data\\scraped_data.csv\")\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Ensure the text is a string\n",
    "        # Remove non-Amharic characters and emojis, keep spaces\n",
    "        cleaned_text = re.sub(r'[^áˆ€-á‰´á€-á\\u1200-\\u137F ]+', '', text)\n",
    "        # Tokenize the text by splitting based on spaces\n",
    "        tokens = cleaned_text.split()\n",
    "        return tokens\n",
    "    return []  # Return empty if it's not a string\n",
    "\n",
    "# Apply the preprocessing function to the 'message' column\n",
    "df['tokens'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Preview the cleaned dataset\n",
    "df[['message', 'tokens']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and tokenized data saved to C:\\Users\\HP\\week 5\\Amharic-Named-Entity-Recognition-\\data\\cleaned_tokenized_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned and tokenized dataset to a CSV file\n",
    "output_path = r\"C:\\Users\\HP\\week 5\\Amharic-Named-Entity-Recognition-\\data\\cleaned_tokenized_data.csv\"\n",
    "df[['message', 'tokens']].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned and tokenized data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\HP\\week 5\\Amharic-Named-Entity-Recognition-\\data\\scraped_data.csv\")\n",
    "\n",
    "# Function to label entities\n",
    "def label_data(text):\n",
    "    tokens = str(text).split()  # Split the text into tokens\n",
    "    labeled_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if 'á‰¥áˆ­'in token:  # Currency\n",
    "            label = 'B-PRICE'\n",
    "        elif token in ['á‰¦áˆŒ', 'áˆ˜áŒˆáŠ“áŠ›','á‹˜ááˆ˜áˆ½','áŒáˆ«áŠ•á‹µ','áˆ°áˆ‹áˆ', 'áŠ á‹²áˆµ áŠ á‰ á‰£']:  # Locations\n",
    "            label = 'B-LOC'\n",
    "        elif token in ['áŠ¥á‰ƒ', 'áŒ«áˆ›','áŠ á‰ á‰£','áˆá‰¥áˆµ']:  # Products\n",
    "            label = 'B-Product'\n",
    "        else:\n",
    "            label = 'O'  # Other tokens\n",
    "\n",
    "        labeled_tokens.append((token, label))\n",
    "\n",
    "    return labeled_tokens\n",
    "\n",
    "# List to store the labeled data\n",
    "labeled_data = []\n",
    "\n",
    "# Process each message in the dataframe\n",
    "for message in df['message']:  # Assuming 'message' is the column containing text\n",
    "    if pd.notna(message):  # Check if the message is not NaN\n",
    "        labeled_sentence = label_data(message)\n",
    "        labeled_data.extend(labeled_sentence)\n",
    "        labeled_data.append((',', 'O'))  # Add a sentence separator\n",
    "\n",
    "# Convert the labeled data into a DataFrame\n",
    "labeled_df = pd.DataFrame(labeled_data, columns=['Token', 'Label'])\n",
    "\n",
    "# Save the labeled data to CSV\n",
    "labeled_df.to_csv(r\"C:\\Users\\HP\\week 5\\Amharic-Named-Entity-Recognition-\\data\\labeled_amharic_data.csv\", index=False)\n",
    "\n",
    "print(\"Labeled data saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
